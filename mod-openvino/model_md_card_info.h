// Auto-generated header containing model card HTML strings
#pragma once

const char* music_generation_music_gen_medium_mono_fp16 = R"md(
<h1>Music Gen Medium Mono (FP16)</h1>
<p>FP16 variant of facebook/musicgen-medium model. This is a mono model, therefore it will produce a mono track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-medium">facebook/musicgen-medium</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_generation_music_gen_medium_mono_int8 = R"md(
<h1>Music Gen Medium Mono (INT8)</h1>
<p>INT8-quantized variant of facebook/musicgen-medium model. This is a mono model, therefore it will produce a mono track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-medium">facebook/musicgen-medium</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_generation_music_gen_small_mono_fp16 = R"md(
<h1>Music Gen Small Mono (FP16)</h1>
<p>FP16 variant of facebook/musicgen-small model. This is a mono model, therefore it will produce a mono track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-small">facebook/musicgen-small</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_generation_music_gen_small_mono_int8 = R"md(
<h1>Music Gen Small Mono (INT8)</h1>
<p>INT8-quantized variant of facebook/musicgen-small model. This is a mono model, therefore it will produce a mono track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-small">facebook/musicgen-small</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_generation_music_gen_small_stereo_fp16 = R"md(
<h1>Music Gen Small Stereo (FP16)</h1>
<p>FP16 variant of facebook/musicgen-stereo-small model. This is a stereo model, therefore it will produce a stereo track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-stereo-small">facebook/musicgen-stereo-small</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_generation_music_gen_small_stereo_int8 = R"md(
<h1>Music Gen Small Stereo (INT8)</h1>
<p>INT8-quantized variant of facebook/musicgen-stereo-small model. This is a stereo model, therefore it will produce a stereo track.</p>
<p>Source Model: <a href="https://huggingface.co/facebook/musicgen-stereo-small">facebook/musicgen-stereo-small</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/musicgen-static-openvino">https://huggingface.co/Intel/musicgen-static-openvino</a> </p>
<p>License: <a href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
)md";

const char* music_restoration_apollo_mp3_jusperlee = R"md(
<h1>Apollo MP3 Restore (@JusperLee)</h1>
<p>An Apollo-based lossy restoration model that works well to restore quality of low-bitrate MP3s.</p>
<p>See <a href="https://github.com/JusperLee">@JusperLee</a> project here: <a href="https://github.com/JusperLee/Apollo/">https://github.com/JusperLee/Apollo/</a></p>
<p>As well as the HuggingFace project, here: <a href="https://huggingface.co/JusperLee/Apollo">https://huggingface.co/JusperLee/Apollo</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: TODO!</p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/cc-by-sa-4.0.md">cc-by-sa-4.0</a></p>
)md";

const char* music_restoration_apollo_universal_lew = R"md(
<h1>Apollo Universal Restore (@Lew)</h1>
<p>An Apollo-based lossy restoration model that works well for any lossy files.</p>
<p>Trained by @Lew.</p>
<p>The pre-trained model (by @Lew) &amp; config was originally uploaded via Discord, and eventually reposted by @deton24 here: <a href="https://github.com/deton24/Lew-s-vocal-enhancer-for-Apollo-by-JusperLee/releases/tag/uni">https://github.com/deton24/Lew-s-vocal-enhancer-for-Apollo-by-JusperLee/releases/tag/uni</a></p>
<p>Unfortunately, since there is no 'official' HuggingFace source repo from @Lew with clear license available, we are unable to provide pre-converted OpenVINO models at this time.</p>
<p>If you're brave, and want to try converting it yourself -- You can use guidance from <a href="https://github.com/RyanMetcalfeInt8/Music-Source-Separation-Training/tree/openvino_conversion/openvino_conversion#convert-apollo-models">here</a></p>
<p>If you do convert it yourself, you should create a <code>apollo_universal</code> folder that contains <code>apollo_fwd.xml</code> &amp; <code>apollo_fwd.bin</code> (output of conversion steps), and place this folder in <code>openvino-models/music_restoration/</code>, and restart Audacity. After this, the model should show up as selectable from the drop-down list.</p>
)md";

const char* music_separation_demucs_v4 = R"md(
<h1>Demucs-v4</h1>
<p>Demucs-v4 is a state-of-the-art music source separation model that can separate drums, bass, vocals, and other stems from any song.</p>
<p>Note that when the <code>(2 Stem) Vocals, Instrumentals</code> mode is selected, the instrumental track is produced by subtracting the vocals from the input track.
<code>instrumental = input_track - vocal_stem</code></p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_demucs_v4_6s = R"md(
<h1>Demucs-v4 6s</h1>
<p>A 6-stem variant of Demucs v4 that can separate an input track into drums, bass, vocals, guitar, piano, and 'other instruments' stems.</p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p><em>Note</em> that this model is considered to be <em>experimental</em>, and it is included here for completeness. To quote the <em>demucs</em> project README from <a href="https://github.com/facebookresearch/demucs?tab=readme-ov-file#demucs-music-source-separation">here</a>: "We are also releasing an experimental 6 sources model, that adds a <code>guitar</code> and <code>piano</code> source. Quick testing seems to show okay quality for <code>guitar</code>, but a lot of bleeding and artifacts for the <code>piano</code> source."</p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_demucs_v4_ft_bass = R"md(
<h1>Demucs-v4 FT Drums</h1>
<p>A FT (fine-tuned) variant of Demucs-v4 that gives slightly better results for bass, as compared with the native version of Demucs-v4.</p>
<p>Note that this model natively produces a single 'bass' stem. The instrumental track, if selected, is produced by subtracting the bass stem from the input track:
<code>instrumental = input_track - bass_stem</code></p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_demucs_v4_ft_drums = R"md(
<h1>Demucs-v4 FT Drums</h1>
<p>A FT (fine-tuned) variant of Demucs-v4 that gives slightly better results for drums, as compared with the 'base' version of Demucs-v4.</p>
<p>Note that this model natively produces a single drums stem. The instrumental track, if selected, is produced by subtracting the drum stem from the input track:
<code>instrumental = input_track - drum_stem</code></p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_demucs_v4_ft_other = R"md(
<h1>Demucs-v4 FT Other Instruments</h1>
<p>A FT (fine-tuned) variant of Demucs-v4 that gives slightly better results for 'other instruments', as compared with the native version of Demucs-v4.</p>
<p>For Demucs-v4, 'Other Instruments' are classified as anything that is <em>not</em> drums, bass, or vocals.</p>
<p>Note that this model natively produces a single 'other instruments' stem. The instrumental track, if selected, is produced by subtracting the 'other instruments' stem from the input track:
<code>instrumental = input_track - other_instruments_stem</code></p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_demucs_v4_ft_vocals = R"md(
<h1>Demucs-v4 FT Vocals</h1>
<p>A FT (fine-tuned) variant of Demucs-v4 that gives slightly better results for vocals, as compared with the 'base' version of Demucs-v4.</p>
<p>Note that this model natively produces a single vocals stem. The instrumental track, if selected, is produced by subtracting the vocal stem from the input track:
<code>instrumental = input_track - vocal_stem</code></p>
<p>For more info, take a look at the source project, here: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch models source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/demucs-openvino">https://huggingface.co/Intel/demucs-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* music_separation_mel_crowd_aufr33_viperx = R"md(
<h1>MelBandRoformer Crowd Extraction / Removal (@aufr33 &amp; @viperx)</h1>
<p>A MelBandRoformer-based crowd extraction / removal model that excels at extracting crowd noise from live recordings. As this is a MelBandRoformer model, it's quite a heavy model so expect long processing times.</p>
<p>Note that this model natively produces a single 'crowd' stem. The 'no crowd' is produced by subtracting the 'crowd' stem from the input track:
<code>no_crowd = input_track - crowd</code></p>
<p>This model was trained by <a href="https://github.com/aufr33">@aufr33</a> &amp; <a href="https://github.com/playdasegunda">@viperx</a></p>
<p>The source pytorch models (checkpoint) &amp; config were originally posted here: <a href="https://github.com/ZFTurbo/Music-Source-Separation-Training/releases/tag/v.1.0.4">https://github.com/ZFTurbo/Music-Source-Separation-Training/releases/tag/v.1.0.4</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: TODO</p>
<p>License: <a href="https://github.com/ZFTurbo/Music-Source-Separation-Training?tab=MIT-1-ov-file#readme">MIT</a></p>
)md";

const char* music_separation_mel_vocals_kimberley_jenson = R"md(
<h1>MelBandRoformer Vocals (@KimberleyJensen)</h1>
<p>A MelBandRoformer-based vocal extraction model that excels at extracting vocals from input tracks. It's a heavier model as compared with <em>Demucs-v4</em>, so expect longer processing times. It might be worth the wait though, as it can produce noticeably better results for certain tracks.</p>
<p>Note that this model natively produces a single vocals stem. The instrumental track, if selected, is produced by subtracting the vocal stem from the input track:
<code>instrumental = input_track - vocal_stem</code></p>
<p>This model was trained by <a href="https://github.com/KimberleyJensen">@KimberleyJensen</a></p>
<p>Take a look at the GitHub project, here: <a href="https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model">https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model</a></p>
<p>Note that there is also a HuggingFace space here: <a href="https://huggingface.co/KimberleyJSN/melbandroformer">https://huggingface.co/KimberleyJSN/melbandroformer</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: TODO</p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/gpl-3.0.md">GPL-3.0</a></p>
)md";

const char* music_separation_msdx23c_drum_sep_jarredou = R"md(
<h1>MDX23C Drum Separation (@jarredou)</h1>
<p>A MDX23C-based drum sepration model that can produce 5 stems: kick, snare, toms, hi-hat, cymbals</p>
<p>As this is a MDX23C model, expect long processing times.</p>
<p>Note that this model natively produces a five stems: kick, snare, toms, hi-hat, &amp; cymbals</p>
<p>There is an additional 'residual' track that is produced, which contains the 'leftover' of whatever is not contained in these five native stems.</p>
<p>It is produced using the following formula:
<code>residual = input_track - (kick + snare + toms + hi-hat + cymbals)</code></p>
<p>This model was trained by <a href="https://github.com/jarredou">@jarredou</a>.</p>
<p>The original pytorch checkpoint / config were downloaded from their GitHub release here: <a href="https://github.com/jarredou/models/releases/tag/DrumSep">https://github.com/jarredou/models/releases/tag/DrumSep</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: TODO</p>
<p>License: <a href="https://github.com/jarredou/models?tab=License-1-ov-file#readme">Attribution-NonCommercial-NoDerivatives 4.0 International</a></p>
)md";

const char* noise_suppression_deepfilternet2 = R"md(
<h1>DeepFilterNet2</h1>
<p>A Noise Suppression model that operates on Full-Band Audio (48kHz) mono tracks. </p>
<p>Note: You can process stereo tracks as well, and in this case the model will be run for the left &amp; right tracks, sequentially.</p>
<p>This is a port of the DeepFilterNet2 features from this project: <a href="https://github.com/Rikorose/DeepFilterNet">https://github.com/Rikorose/DeepFilterNet</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and posted here: <a href="https://huggingface.co/Intel/deepfilternet-openvino">https://huggingface.co/Intel/deepfilternet-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* noise_suppression_deepfilternet3 = R"md(
<h1>DeepFilterNet3</h1>
<p>A Noise Suppression model that operates on Full-Band Audio (48kHz) mono tracks.</p>
<p>Note: You can process stereo tracks as well, and in this case the model will be run for the left &amp; right tracks, sequentially.</p>
<p>This is a port of the DeepFilterNet3 features from this project: <a href="https://github.com/Rikorose/DeepFilterNet">https://github.com/Rikorose/DeepFilterNet</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and posted here: <a href="https://huggingface.co/Intel/deepfilternet-openvino">https://huggingface.co/Intel/deepfilternet-openvino</a></p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md">MIT</a></p>
)md";

const char* noise_suppression_denseunet = R"md(
<h1>DenseUNet</h1>
<p>This is a model for noise suppression to make speech cleaner, and operates with 16khz mono audio tracks.</p>
<p>For more information about this model, see <a href="https://docs.openvino.ai/2023.3/omz_models_model_noise_suppression_denseunet_ll_0001.html">here</a></p>
<p>This model is stored in OpenVINO's Open Model Zoo repositories, here: <a href="https://storage.openvinotoolkit.org/repositories/open_model_zoo/2023.0/models_bin/1/noise-suppression-denseunet-ll-0001/">https://storage.openvinotoolkit.org/repositories/open_model_zoo/2023.0/models_bin/1/noise-suppression-denseunet-ll-0001/</a></p>
)md";

const char* reverb_removal_mel_band_dereverb_mono_anvuew = R"md(
<h1>MelBandRoformer Dereverb Mono (@anvuew)</h1>
<p>A MelBandRoformer-based model trained to extract / remove reverb from vocals. </p>
<p>This particular (mono) variant seems to work well for spoken audio as well, even though it's primarily trained on vocal tracks (i.e. music). In the case of spoken audio tracks, you can try running directly on a mono or stereo track. </p>
<p>We've noticed that for some spoken tracks with only <em>minor</em> reverb present, sometimes this model doesn't end up removing the reverb. This might seem odd, but in these cases you can try to <em>add</em> some additional reverb to the source track (<strong>Effects-&gt;Delay and Reverb-&gt;Reverb..</strong>), and then re-apply the <strong>Reverb Removal</strong> effect.</p>
<p>As this is a MelBandRoformer model, so expect long processing times.</p>
<p>This model was trained by <a href="https://huggingface.co/anvuew">@anvuew</a>.</p>
<p>The original pytorch checkpoint / config were downloaded from HuggingFace here: <a href="https://huggingface.co/anvuew/dereverb_mel_band_roformer">https://huggingface.co/anvuew/dereverb_mel_band_roformer</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: TODO</p>
<p>License: <a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/gpl-3.0.md">GPL-3.0</a></p>
)md";

const char* super_resolution_basic_general = R"md(
<h1>Basic (General) (FP16)</h1>
<p>This super resolution model is intended to be used for enhancing all types of audio, including music and environmental sounds.</p>
<p>To understand more about how to use the <em>Super Resolution</em>, you can read the documentation <a href="https://github.com/intel/openvino-plugins-ai-audacity/blob/main/doc/feature_doc/super_resolution/README.md">here</a></p>
<p>You can also watch the YouTube video that desribes a bit more about how this feature works:<br />
<a href="https://youtu.be/StrQef9lLMk?si=ba7V0SiRrbxjf5Of">Audio Super Resolution on your AI PC | AI with Guy | Intel Software</a></p>
<p>Note that this effect is a port of this project: <a href="https://github.com/haoheliu/versatile_audio_super_resolution">https://github.com/haoheliu/versatile_audio_super_resolution</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/versatile_audio_super_resolution_openvino">https://huggingface.co/Intel/versatile_audio_super_resolution_openvino</a></p>
<p>License: <a href="https://github.com/haoheliu/versatile_audio_super_resolution?tab=MIT-1-ov-file#readme">MIT</a></p>
)md";

const char* super_resolution_speech = R"md(
<h1>Speech (FP16)</h1>
<p>This super resolution model is intended to be used for enhancing audio with isolated speech.</p>
<p>To understand more about how to use the <em>Super Resolution</em>, you can read the documentation <a href="https://github.com/intel/openvino-plugins-ai-audacity/blob/main/doc/feature_doc/super_resolution/README.md">here</a></p>
<p>You can also watch the YouTube video that desribes a bit more about how this feature works:<br />
<a href="https://youtu.be/StrQef9lLMk?si=ba7V0SiRrbxjf5Of">Audio Super Resolution on your AI PC | AI with Guy | Intel Software</a></p>
<p>Note that this effect is a port of this project: <a href="https://github.com/haoheliu/versatile_audio_super_resolution">https://github.com/haoheliu/versatile_audio_super_resolution</a></p>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, the pytorch source models were converted to OpenVINO IR format, and stored here: <a href="https://huggingface.co/Intel/versatile_audio_super_resolution_openvino">https://huggingface.co/Intel/versatile_audio_super_resolution_openvino</a></p>
<p>License: <a href="https://github.com/haoheliu/versatile_audio_super_resolution?tab=MIT-1-ov-file#readme">MIT</a></p>
)md";

const char* whisper_transcription_info = R"md(
<h2>Quantization Guide</h2>
<p>The Whisper models available to use with this project come in three <em>quantization</em> variants: <em>FP16</em>, <em>INT8</em>, &amp; <em>INT4</em>.</p>
<p><strong>FP16</strong>: Floating-Point 16 (i.e. half precision, 16-bit). Highest memory footprint &amp; utilization, but gives highest accuracy (transcription quality) in most cases. Lowest performance (on most systems).  </p>
<p><strong>INT8</strong>: Integer-8 (8-bit) quantized. Roughly 1/2 of the memory footprint / utilization as compared with FP16. Improved performance on most systems, as compared with <em>FP16</em>. Minimal loss in accuracy (transcription quality) as compared with <em>FP16</em>.  </p>
<p><strong>INT4</strong>: Integer-4 (4-bit) quantized. Roughly 1/4 of the memory footprint / utilization as compared with FP16. Improved performance on most systems, as compared with <em>INT8</em> &amp; <em>FP16</em>. Minimal loss in accuracy (transcription quality) as compared with <em>INT8</em> &amp; <em>FP16</em>.  </p>
<h2>Model Variant Guide</h2>
<p><strong>Note</strong>: The following guide has used / copied / modified some of the information found here: <a href="https://whisper-api.com/blog/models/">https://whisper-api.com/blog/models/</a></p>
<p>There are many variants of whisper models, each one representing a tradeoff between accuracy, speed, and resource requirements.</p>
<h3>Base:</h3>
<p>A 74M parameter multilingual model.
<strong>Multi-Language Support</strong>: Yes<br />
<strong>Best for</strong>: General purpose transcription with reasonable accuracy when resources are limited. This is a great balance between speed and accuracy.  </p>
<h3>Small:</h3>
<p>A 244M parameter multilingual model.<br />
<strong>Multi-Language Support</strong>: Yes<br />
<strong>Best for</strong>: Daily transcription needs with good accuracy and reasonable speed. More accurate than base model but requires more resources.  </p>
<h3>Medium:</h3>
<p>A 769M parameter multilingual model.<br />
<strong>Multi-Language Support</strong>: Yes<br />
<strong>Best for</strong>: High-quality transcriptions where accuracy is important and you have decent computing resources.  </p>
<h3>Large:</h3>
<p>There are (currently) three variants of Whisper-large:<br />
<strong>large-v1</strong>: Original large model (1.5B parameters)<br />
<strong>large-v2</strong>: Improved large model<br />
<strong>large-v3</strong>: Latest larger model with the best accuracy  (recommended). </p>
<p><strong>Multi-Language Support</strong>: Yes<br />
<strong>Best for</strong>: Professional transcription where maximum accuracy is essential, especially for <em>multi-language</em> transcriptions. </p>
<h3>Distil-Large-V3:</h3>
<p>A <em>distilled</em> (756M parameter) version of the <strong>large-v3</strong> model, which can produce <em>large-v3</em>-like quality but at a dramatically reduced memory &amp; processing cost. <strong>But</strong>.. it can only transcribe <em>english</em> recordings.  </p>
<p><strong>Multi-Language Support</strong>: No (English-only transcription)<br />
<strong>Best for</strong>: Professional <em>english</em> transcription where maximum accuracy is essential.  </p>
<h2>How to use</h2>
<p>There is some documentation <a href="https://github.com/intel/openvino-plugins-ai-audacity/tree/main/doc/feature_doc/whisper_transcription">here</a> about how to use these plugins.</p>
<h2>OpenVINO Speech-to-Text collection</h2>
<p>For use with <a href="https://github.com/intel/openvino-plugins-ai-audacity">OpenVINO AI Plugins for Audacity</a>, all of these models are downloaded from the OpenVINO's <a href="https://huggingface.co/collections/OpenVINO/speech-to-text-672321d5c070537a178a8aeb">Speech-to-Text</a> collection on HuggingFace.</p>
<h2>License:</h2>
<p>For <em>Distil</em> variants: <a href="https://github.com/huggingface/distil-whisper/blob/main/LICENSE">MIT</a>  </p>
<p>For <em>Base</em>, <em>Small</em>, <em>Medium</em>, and <em>Large</em> variants: <a href="https://choosealicense.com/licenses/apache-2.0/">Apache-2.0</a>  </p>
)md";

